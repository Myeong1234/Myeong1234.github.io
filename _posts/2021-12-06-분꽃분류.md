---
title: "분꽃 분류"
excerpt_separator: "<!--more-->"
categories:
  - Post Formats
tags:
   - python
   - 데이터 분석
---





# 분꽃 분류


```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
import pandas as pd
```


```python
iris = load_iris()
iris.target[[10, 50, 100]]
```




    array([0, 1, 2])




```python
list(iris.target_names)
```




    ['setosa', 'versicolor', 'virginica']




```python
iris
```




    {'data': array([[5.1, 3.5, 1.4, 0.2],
            [4.9, 3. , 1.4, 0.2],
            [4.7, 3.2, 1.3, 0.2],
            [4.6, 3.1, 1.5, 0.2],
            [5. , 3.6, 1.4, 0.2],
            [5.4, 3.9, 1.7, 0.4],
            [4.6, 3.4, 1.4, 0.3],
            [5. , 3.4, 1.5, 0.2],
            [4.4, 2.9, 1.4, 0.2],
            [4.9, 3.1, 1.5, 0.1],
            [5.4, 3.7, 1.5, 0.2],
            [4.8, 3.4, 1.6, 0.2],
            [4.8, 3. , 1.4, 0.1],
            [4.3, 3. , 1.1, 0.1],
            [5.8, 4. , 1.2, 0.2],
            [5.7, 4.4, 1.5, 0.4],
            [5.4, 3.9, 1.3, 0.4],
            [5.1, 3.5, 1.4, 0.3],
            [5.7, 3.8, 1.7, 0.3],
            [5.1, 3.8, 1.5, 0.3],
            [5.4, 3.4, 1.7, 0.2],
            [5.1, 3.7, 1.5, 0.4],
            [4.6, 3.6, 1. , 0.2],
            [5.1, 3.3, 1.7, 0.5],
            [4.8, 3.4, 1.9, 0.2],
            [5. , 3. , 1.6, 0.2],
            [5. , 3.4, 1.6, 0.4],
            [5.2, 3.5, 1.5, 0.2],
            [5.2, 3.4, 1.4, 0.2],
            [4.7, 3.2, 1.6, 0.2],
            [4.8, 3.1, 1.6, 0.2],
            [5.4, 3.4, 1.5, 0.4],
            [5.2, 4.1, 1.5, 0.1],
            [5.5, 4.2, 1.4, 0.2],
            [4.9, 3.1, 1.5, 0.2],
            [5. , 3.2, 1.2, 0.2],
            [5.5, 3.5, 1.3, 0.2],
            [4.9, 3.6, 1.4, 0.1],
            [4.4, 3. , 1.3, 0.2],
            [5.1, 3.4, 1.5, 0.2],
            [5. , 3.5, 1.3, 0.3],
            [4.5, 2.3, 1.3, 0.3],
            [4.4, 3.2, 1.3, 0.2],
            [5. , 3.5, 1.6, 0.6],
            [5.1, 3.8, 1.9, 0.4],
            [4.8, 3. , 1.4, 0.3],
            [5.1, 3.8, 1.6, 0.2],
            [4.6, 3.2, 1.4, 0.2],
            [5.3, 3.7, 1.5, 0.2],
            [5. , 3.3, 1.4, 0.2],
            [7. , 3.2, 4.7, 1.4],
            [6.4, 3.2, 4.5, 1.5],
            [6.9, 3.1, 4.9, 1.5],
            [5.5, 2.3, 4. , 1.3],
            [6.5, 2.8, 4.6, 1.5],
            [5.7, 2.8, 4.5, 1.3],
            [6.3, 3.3, 4.7, 1.6],
            [4.9, 2.4, 3.3, 1. ],
            [6.6, 2.9, 4.6, 1.3],
            [5.2, 2.7, 3.9, 1.4],
            [5. , 2. , 3.5, 1. ],
            [5.9, 3. , 4.2, 1.5],
            [6. , 2.2, 4. , 1. ],
            [6.1, 2.9, 4.7, 1.4],
            [5.6, 2.9, 3.6, 1.3],
            [6.7, 3.1, 4.4, 1.4],
            [5.6, 3. , 4.5, 1.5],
            [5.8, 2.7, 4.1, 1. ],
            [6.2, 2.2, 4.5, 1.5],
            [5.6, 2.5, 3.9, 1.1],
            [5.9, 3.2, 4.8, 1.8],
            [6.1, 2.8, 4. , 1.3],
            [6.3, 2.5, 4.9, 1.5],
            [6.1, 2.8, 4.7, 1.2],
            [6.4, 2.9, 4.3, 1.3],
            [6.6, 3. , 4.4, 1.4],
            [6.8, 2.8, 4.8, 1.4],
            [6.7, 3. , 5. , 1.7],
            [6. , 2.9, 4.5, 1.5],
            [5.7, 2.6, 3.5, 1. ],
            [5.5, 2.4, 3.8, 1.1],
            [5.5, 2.4, 3.7, 1. ],
            [5.8, 2.7, 3.9, 1.2],
            [6. , 2.7, 5.1, 1.6],
            [5.4, 3. , 4.5, 1.5],
            [6. , 3.4, 4.5, 1.6],
            [6.7, 3.1, 4.7, 1.5],
            [6.3, 2.3, 4.4, 1.3],
            [5.6, 3. , 4.1, 1.3],
            [5.5, 2.5, 4. , 1.3],
            [5.5, 2.6, 4.4, 1.2],
            [6.1, 3. , 4.6, 1.4],
            [5.8, 2.6, 4. , 1.2],
            [5. , 2.3, 3.3, 1. ],
            [5.6, 2.7, 4.2, 1.3],
            [5.7, 3. , 4.2, 1.2],
            [5.7, 2.9, 4.2, 1.3],
            [6.2, 2.9, 4.3, 1.3],
            [5.1, 2.5, 3. , 1.1],
            [5.7, 2.8, 4.1, 1.3],
            [6.3, 3.3, 6. , 2.5],
            [5.8, 2.7, 5.1, 1.9],
            [7.1, 3. , 5.9, 2.1],
            [6.3, 2.9, 5.6, 1.8],
            [6.5, 3. , 5.8, 2.2],
            [7.6, 3. , 6.6, 2.1],
            [4.9, 2.5, 4.5, 1.7],
            [7.3, 2.9, 6.3, 1.8],
            [6.7, 2.5, 5.8, 1.8],
            [7.2, 3.6, 6.1, 2.5],
            [6.5, 3.2, 5.1, 2. ],
            [6.4, 2.7, 5.3, 1.9],
            [6.8, 3. , 5.5, 2.1],
            [5.7, 2.5, 5. , 2. ],
            [5.8, 2.8, 5.1, 2.4],
            [6.4, 3.2, 5.3, 2.3],
            [6.5, 3. , 5.5, 1.8],
            [7.7, 3.8, 6.7, 2.2],
            [7.7, 2.6, 6.9, 2.3],
            [6. , 2.2, 5. , 1.5],
            [6.9, 3.2, 5.7, 2.3],
            [5.6, 2.8, 4.9, 2. ],
            [7.7, 2.8, 6.7, 2. ],
            [6.3, 2.7, 4.9, 1.8],
            [6.7, 3.3, 5.7, 2.1],
            [7.2, 3.2, 6. , 1.8],
            [6.2, 2.8, 4.8, 1.8],
            [6.1, 3. , 4.9, 1.8],
            [6.4, 2.8, 5.6, 2.1],
            [7.2, 3. , 5.8, 1.6],
            [7.4, 2.8, 6.1, 1.9],
            [7.9, 3.8, 6.4, 2. ],
            [6.4, 2.8, 5.6, 2.2],
            [6.3, 2.8, 5.1, 1.5],
            [6.1, 2.6, 5.6, 1.4],
            [7.7, 3. , 6.1, 2.3],
            [6.3, 3.4, 5.6, 2.4],
            [6.4, 3.1, 5.5, 1.8],
            [6. , 3. , 4.8, 1.8],
            [6.9, 3.1, 5.4, 2.1],
            [6.7, 3.1, 5.6, 2.4],
            [6.9, 3.1, 5.1, 2.3],
            [5.8, 2.7, 5.1, 1.9],
            [6.8, 3.2, 5.9, 2.3],
            [6.7, 3.3, 5.7, 2.5],
            [6.7, 3. , 5.2, 2.3],
            [6.3, 2.5, 5. , 1.9],
            [6.5, 3. , 5.2, 2. ],
            [6.2, 3.4, 5.4, 2.3],
            [5.9, 3. , 5.1, 1.8]]),
     'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
            2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
            2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),
     'frame': None,
     'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),
     'DESCR': '.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n                \n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher\'s paper. Note that it\'s the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher\'s paper is a classic in the field and\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n.. topic:: References\n\n   - Fisher, R.A. "The use of multiple measurements in taxonomic problems"\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in "Contributions to\n     Mathematical Statistics" (John Wiley, NY, 1950).\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n   - Dasarathy, B.V. (1980) "Nosing Around the Neighborhood: A New System\n     Structure and Classification Rule for Recognition in Partially Exposed\n     Environments".  IEEE Transactions on Pattern Analysis and Machine\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n   - Gates, G.W. (1972) "The Reduced Nearest Neighbor Rule".  IEEE Transactions\n     on Information Theory, May 1972, 431-433.\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al"s AUTOCLASS II\n     conceptual clustering system finds 3 classes in the data.\n   - Many, many more ...',
     'feature_names': ['sepal length (cm)',
      'sepal width (cm)',
      'petal length (cm)',
      'petal width (cm)'],
     'filename': 'iris.csv',
     'data_module': 'sklearn.datasets.data'}



### DataFrame으로 만들기


```python
iris_data = iris.data
iris_label = iris.target
print('iris target값 :', iris_label)
print('iris target명 :', iris.target_names)
```

    iris target값 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
     2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
     2 2]
    iris target명 : ['setosa' 'versicolor' 'virginica']



```python
iris_df = pd.DataFrame(data = iris_data, columns = iris.feature_names)
iris_df['label'] = iris.target
iris_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>145</th>
      <td>6.7</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>146</th>
      <td>6.3</td>
      <td>2.5</td>
      <td>5.0</td>
      <td>1.9</td>
      <td>2</td>
    </tr>
    <tr>
      <th>147</th>
      <td>6.5</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>148</th>
      <td>6.2</td>
      <td>3.4</td>
      <td>5.4</td>
      <td>2.3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>149</th>
      <td>5.9</td>
      <td>3.0</td>
      <td>5.1</td>
      <td>1.8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>150 rows × 5 columns</p>
</div>




```python
iris_df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 150 entries, 0 to 149
    Data columns (total 5 columns):
     #   Column             Non-Null Count  Dtype  
    ---  ------             --------------  -----  
     0   sepal length (cm)  150 non-null    float64
     1   sepal width (cm)   150 non-null    float64
     2   petal length (cm)  150 non-null    float64
     3   petal width (cm)   150 non-null    float64
     4   label              150 non-null    int32  
    dtypes: float64(4), int32(1)
    memory usage: 5.4 KB


####.describe() 요약해주는 함수


```python
iris_df.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>150.000000</td>
      <td>150.000000</td>
      <td>150.000000</td>
      <td>150.000000</td>
      <td>150.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>5.843333</td>
      <td>3.057333</td>
      <td>3.758000</td>
      <td>1.199333</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.828066</td>
      <td>0.435866</td>
      <td>1.765298</td>
      <td>0.762238</td>
      <td>0.819232</td>
    </tr>
    <tr>
      <th>min</th>
      <td>4.300000</td>
      <td>2.000000</td>
      <td>1.000000</td>
      <td>0.100000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>5.100000</td>
      <td>2.800000</td>
      <td>1.600000</td>
      <td>0.300000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>5.800000</td>
      <td>3.000000</td>
      <td>4.350000</td>
      <td>1.300000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>6.400000</td>
      <td>3.300000</td>
      <td>5.100000</td>
      <td>1.800000</td>
      <td>2.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>7.900000</td>
      <td>4.400000</td>
      <td>6.900000</td>
      <td>2.500000</td>
      <td>2.000000</td>
    </tr>
  </tbody>
</table>
</div>



### train set 과 test set 으로 분리


```python
len([x for x in iris_df['label'] if x == 0])
```




    50



#### sklearn.model_selection.train_test_split
* sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)


```python
X_train, X_test, Y_train, Y_test = train_test_split(iris_data, iris.target, test_size=0.2, random_state=18)
```

### 의사결정나무(DecisionTree)를 이용하여 학습

#### sklearn.tree.DecisionTreeClassifier
* class sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)

* fit(x_train, y_train)     ## 모수 추정(estimat)
* get_params()              ## 추정된 모수 확인
* predict(x_test)           ## x_test로부터 라벨 예측
* predict_log_proba(x_test) ## 로그 취한 확률 예측
* predict_proba(x_test)     ## 각 라벨로 예측될 확률
* score(x_test, y_test)     ## 모델 정확도 평가를 위한 mean accuracy


```python
#DecisionTreeClassifier 객체 생성
dt_clf = DecisionTreeClassifier(random_state=18)
#학습 수행
dt_clf.fit(X_train, Y_train)
```




    DecisionTreeClassifier(random_state=18)




```python
# 학습이 완료된 DecisionTreeClassifier 객체에서 테스트 데이터 세트로 예측 수행
pred = dt_clf.predict(X_test)
#pred = dt_clf.predict_proba(X_test)
pred
```




    array([1, 1, 1, 0, 0, 0, 1, 0, 2, 1, 2, 1, 0, 2, 0, 1, 0, 2, 0, 0, 1, 2,
           2, 1, 2, 0, 0, 0, 2, 2])




```python
Y_test
```




    array([1, 1, 1, 0, 0, 0, 2, 0, 2, 1, 2, 1, 0, 2, 0, 1, 0, 2, 0, 0, 1, 2,
           2, 1, 2, 0, 0, 0, 2, 2])



### 트리 그려보기


```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

# 데이터 로드
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=11)

# 모델 학습
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)
```




    DecisionTreeClassifier(random_state=42)




```python
# 결정트리 규칙을 시각화
import matplotlib.pyplot as plt
from sklearn import tree

plt.figure( figsize=(20,15) )
tree.plot_tree(model, 
               class_names=iris.target_names,
               feature_names=iris.feature_names,
               impurity=True, filled=True,
               rounded=True)
```




    [Text(343.38461538461536, 747.4499999999999, 'petal length (cm) <= 2.45\ngini = 0.667\nsamples = 120\nvalue = [41, 40, 39]\nclass = setosa'),
     Text(257.53846153846155, 611.55, 'gini = 0.0\nsamples = 41\nvalue = [41, 0, 0]\nclass = setosa'),
     Text(429.23076923076917, 611.55, 'petal width (cm) <= 1.55\ngini = 0.5\nsamples = 79\nvalue = [0, 40, 39]\nclass = versicolor'),
     Text(171.69230769230768, 475.65, 'petal length (cm) <= 5.25\ngini = 0.051\nsamples = 38\nvalue = [0, 37, 1]\nclass = versicolor'),
     Text(85.84615384615384, 339.74999999999994, 'gini = 0.0\nsamples = 37\nvalue = [0, 37, 0]\nclass = versicolor'),
     Text(257.53846153846155, 339.74999999999994, 'gini = 0.0\nsamples = 1\nvalue = [0, 0, 1]\nclass = virginica'),
     Text(686.7692307692307, 475.65, 'petal width (cm) <= 1.75\ngini = 0.136\nsamples = 41\nvalue = [0, 3, 38]\nclass = virginica'),
     Text(429.23076923076917, 339.74999999999994, 'sepal width (cm) <= 2.6\ngini = 0.5\nsamples = 4\nvalue = [0, 2, 2]\nclass = versicolor'),
     Text(343.38461538461536, 203.8499999999999, 'gini = 0.0\nsamples = 1\nvalue = [0, 0, 1]\nclass = virginica'),
     Text(515.0769230769231, 203.8499999999999, 'petal length (cm) <= 5.45\ngini = 0.444\nsamples = 3\nvalue = [0, 2, 1]\nclass = versicolor'),
     Text(429.23076923076917, 67.94999999999993, 'gini = 0.0\nsamples = 2\nvalue = [0, 2, 0]\nclass = versicolor'),
     Text(600.9230769230769, 67.94999999999993, 'gini = 0.0\nsamples = 1\nvalue = [0, 0, 1]\nclass = virginica'),
     Text(944.3076923076923, 339.74999999999994, 'petal length (cm) <= 4.85\ngini = 0.053\nsamples = 37\nvalue = [0, 1, 36]\nclass = virginica'),
     Text(858.4615384615383, 203.8499999999999, 'sepal length (cm) <= 5.95\ngini = 0.444\nsamples = 3\nvalue = [0, 1, 2]\nclass = virginica'),
     Text(772.6153846153845, 67.94999999999993, 'gini = 0.0\nsamples = 1\nvalue = [0, 1, 0]\nclass = versicolor'),
     Text(944.3076923076923, 67.94999999999993, 'gini = 0.0\nsamples = 2\nvalue = [0, 0, 2]\nclass = virginica'),
     Text(1030.1538461538462, 203.8499999999999, 'gini = 0.0\nsamples = 34\nvalue = [0, 0, 34]\nclass = virginica')]




​    
![png](\github_img\iris\output_23_1.png)
​    


#### sklearn.metrics.accuracy_score
*sklearn.metrics.accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None)


```python
from sklearn.metrics import accuracy_score
print(f'예측정확도 : {accuracy_score(Y_test,pred)}')
```

    예측정확도 : 0.9666666666666667



```python
dt_clf.predict(np.array([6.5,3.0,5.2,2.0]).reshape(1,-1))
```




    array([2])




```python
from sklearn.datasets import make_classification

data = make_classification()

X = data[0]
Y =  data[1]

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
model = DecisionTreeClassifier(max_depth=3,random_state=42)
model.fit(X_train, y_train)
pred = model.predict(X_test)
pred

```




    array([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0])




```python
y_test
accuracy = accuracy_score(y_test, pred)
print(f"예측 정확도는 {accuracy}입니다.")
```

    예측 정확도는 0.8입니다.



```python
from sklearn.datasets import make_blobs
make_blobs()
```




    (array([[ 5.21386443, -3.981473  ],
            [ 7.33223123, -5.00306971],
            [ 4.85758725,  7.7229724 ],
            [ 3.2990924 , 10.85450263],
            [ 5.82066716, -3.95200304],
            [ 5.72417197, -3.40194305],
            [ 7.4502438 , -3.05374879],
            [ 8.30022522, -4.80066612],
            [ 7.24463742,  8.87219466],
            [ 6.24997815,  9.25093205],
            [ 6.45565163, -4.09787962],
            [ 7.57446119, -1.83740291],
            [ 7.01223494, -4.15859833],
            [ 4.84993199,  8.57332911],
            [ 6.61994904,  9.28033803],
            [ 6.442583  ,  9.30376537],
            [ 5.29039726,  9.69717336],
            [ 7.28019951, -5.00558964],
            [ 3.78870485,  8.78970625],
            [ 6.53412602, -2.57508625],
            [ 7.38355835, -2.06069431],
            [ 8.60956353, -4.7183631 ],
            [ 8.65344594, -5.6522803 ],
            [ 6.81434926, -3.37761398],
            [ 6.78418462, -4.7346242 ],
            [ 6.84748051,  7.67992688],
            [ 4.61232305,  9.55810655],
            [ 7.13593814, -2.14105781],
            [ 6.72011727, -2.85494239],
            [ 4.46538246,  8.94453075],
            [ 6.75555963, -2.0725233 ],
            [ 5.80017842,  8.3272    ],
            [ 7.0685936 , -4.0573639 ],
            [ 7.4293272 , -3.34257225],
            [ 9.07203018, -5.58510705],
            [ 7.8885447 , -3.57705505],
            [ 7.02352299, -3.84472083],
            [ 9.29960968, -4.21899596],
            [ 5.8731323 ,  6.78833288],
            [ 7.747636  , -4.16225927],
            [ 8.03775243, -4.67043961],
            [ 7.97806744, -3.37812745],
            [ 5.47717011,  9.73215478],
            [ 4.97864247,  8.38703682],
            [ 6.18548523,  8.02308795],
            [ 7.41228436, -2.15300614],
            [ 6.9611774 , -4.70444176],
            [ 6.49528288, -4.71073782],
            [ 5.99064989,  9.30539   ],
            [ 5.52412449, -3.38791895],
            [ 7.03816711, -4.03818187],
            [ 7.96342518, -3.66237687],
            [ 9.12896635, -5.32779268],
            [ 5.3530521 ,  7.31379366],
            [ 5.30806774,  8.84441471],
            [ 8.16735013, -4.68876792],
            [ 5.96698118,  8.04498125],
            [ 5.89537941, -5.67919637],
            [ 6.60965678,  9.74177267],
            [ 5.06643198,  9.45073462],
            [ 7.85352924, -3.84860494],
            [ 6.83323961, -3.05480132],
            [ 5.62613649,  8.13858662],
            [ 3.40871366,  7.44632861],
            [ 5.50060616, -3.00430315],
            [ 7.466121  , -2.55931356],
            [ 6.6512971 , -4.67387064],
            [ 5.1543511 , 10.18415061],
            [ 4.54355487,  7.86409338],
            [ 6.6580323 , -2.73388539],
            [ 4.31046838, -3.22886728],
            [ 8.15643134, -3.68522642],
            [ 7.73754622, -3.00953853],
            [ 6.9922717 ,  9.33619905],
            [ 4.94010633, -2.94387693],
            [ 4.43865129,  7.31841264],
            [ 7.31656122, -2.88079641],
            [ 8.89782239, -3.9302513 ],
            [ 7.33541578, -4.38245403],
            [ 7.57226485, -3.34711326],
            [ 9.02284004, -4.01372353],
            [10.34601742, -5.10613852],
            [ 8.50436553, -4.35092212],
            [ 4.69482337,  8.75517757],
            [ 8.4386803 , -3.90078152],
            [ 7.57211495, -5.18700583],
            [ 7.96225364, -5.54283157],
            [ 4.02981895, -3.59205505],
            [ 9.22719921, -5.14824768],
            [ 5.20269953,  7.55873267],
            [ 4.23569927,  8.85930568],
            [ 5.84821792,  8.69241699],
            [10.87270436, -5.67892092],
            [ 6.23622406, -4.06015754],
            [ 6.12276132,  7.66074486],
            [ 7.44499044, -2.97416443],
            [ 6.9127065 , -3.82073014],
            [ 8.1124105 , -3.17734273],
            [ 4.0799189 , -2.53393379],
            [ 7.44399954, -3.11047486]]),
     array([2, 1, 0, 0, 2, 2, 2, 1, 0, 0, 2, 2, 1, 0, 0, 0, 0, 1, 0, 2, 2, 1,
            1, 2, 2, 0, 0, 2, 1, 0, 2, 0, 2, 1, 1, 1, 1, 1, 0, 1, 1, 2, 0, 0,
            0, 2, 2, 1, 0, 2, 1, 2, 1, 0, 0, 1, 0, 1, 0, 0, 2, 2, 0, 0, 2, 2,
            1, 0, 0, 2, 2, 1, 2, 0, 2, 0, 2, 1, 2, 2, 1, 1, 1, 0, 1, 1, 1, 2,
            1, 0, 0, 0, 1, 2, 0, 1, 1, 1, 2, 2]))




```python
from sklearn.model_selection import KFold

iris = load_iris()
features = iris.data
label = iris.target
dt_clf = DecisionTreeClassifier(random_state = 156)

kfold = KFold(n_splits = 5)
cv_accuracy = []
print('붗꽃 데이터 세트 크기', features.shape[0])

```

    붗꽃 데이터 세트 크기 150



```python
n_iter = 0

for train_index, test_index in kfold.split(features) :
    X_train, X_test = features[train_index],features[test_index]
    Y_train, Y_test = label[train_index],label[test_index]
    
    dt_clf.fit(X_train,Y_train)
    pred = dt_clf.predict(X_test)
    n_iter += 1
    
    accuracy = np.round(accuracy_score(Y_test,pred), 4)
    train_size = X_train.shape[0]
    test_size = X_test.shape[0]
    print(f'{n_iter} 교차 검증 정확도 : {accuracy} 학습데이터 크기 : {train_size} 검증데이터 크기 : {test_size}')
    print(f'{n_iter} 검증 세트 인덱스 : {test_index}')
    cv_accuracy.append(accuracy)
    
print(f'평균 검증 정확도 : {np.mean(cv_accuracy)}')
```

    1 교차 검증 정확도 : 1.0 학습데이터 크기 : 120 검증데이터 크기 : 30
    1 검증 세트 인덱스 : [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
     24 25 26 27 28 29]
    2 교차 검증 정확도 : 0.9667 학습데이터 크기 : 120 검증데이터 크기 : 30
    2 검증 세트 인덱스 : [30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53
     54 55 56 57 58 59]
    3 교차 검증 정확도 : 0.8667 학습데이터 크기 : 120 검증데이터 크기 : 30
    3 검증 세트 인덱스 : [60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83
     84 85 86 87 88 89]
    4 교차 검증 정확도 : 0.9333 학습데이터 크기 : 120 검증데이터 크기 : 30
    4 검증 세트 인덱스 : [ 90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
     108 109 110 111 112 113 114 115 116 117 118 119]
    5 교차 검증 정확도 : 0.7333 학습데이터 크기 : 120 검증데이터 크기 : 30
    5 검증 세트 인덱스 : [120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137
     138 139 140 141 142 143 144 145 146 147 148 149]
    평균 검증 정확도 : 0.9





```python
iris = load_iris()

X = iris.data
Y = iris.target

```


```python
from sklearn.model_selection import cross_val_score

scorce = cross_val_score(model, X, Y, cv =5)
```


```python
scorce.mean()
```




    0.9533333333333334




```python
scorce.std()
```




    0.03399346342395189




```python
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(random_state = 11)

cross_val_score(model, X, Y, cv =5)
```

    D:\Program Files\Python39\lib\site-packages\sklearn\linear_model\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      n_iter_i = _check_optimize_result(





    array([0.96666667, 1.        , 0.93333333, 0.96666667, 1.        ])



