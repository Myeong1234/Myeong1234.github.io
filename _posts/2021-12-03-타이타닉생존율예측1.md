---
title: "타이타닉 생존율 예측(3가지 모델)"
toc: true
toc_sticky: true
excerpt_separator: "<!--more-->"
categories:
  - Post Formats
tags:
   - python
   - 분석
---

# 타이타닉 생존율 예측1


```python
import pandas as pd

filename ='/content/drive/MyDrive/muchin Learning/titanic1.csv'
titanic = pd.read_csv(filename)
titanic
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }


    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }

</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>C</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>female</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>C123</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>male</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>886</th>
      <td>887</td>
      <td>0</td>
      <td>2</td>
      <td>Montvila, Rev. Juozas</td>
      <td>male</td>
      <td>27.0</td>
      <td>0</td>
      <td>0</td>
      <td>211536</td>
      <td>13.0000</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>887</th>
      <td>888</td>
      <td>1</td>
      <td>1</td>
      <td>Graham, Miss. Margaret Edith</td>
      <td>female</td>
      <td>19.0</td>
      <td>0</td>
      <td>0</td>
      <td>112053</td>
      <td>30.0000</td>
      <td>B42</td>
      <td>S</td>
    </tr>
    <tr>
      <th>888</th>
      <td>889</td>
      <td>0</td>
      <td>3</td>
      <td>Johnston, Miss. Catherine Helen "Carrie"</td>
      <td>female</td>
      <td>NaN</td>
      <td>1</td>
      <td>2</td>
      <td>W./C. 6607</td>
      <td>23.4500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>889</th>
      <td>890</td>
      <td>1</td>
      <td>1</td>
      <td>Behr, Mr. Karl Howell</td>
      <td>male</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>111369</td>
      <td>30.0000</td>
      <td>C148</td>
      <td>C</td>
    </tr>
    <tr>
      <th>890</th>
      <td>891</td>
      <td>0</td>
      <td>3</td>
      <td>Dooley, Mr. Patrick</td>
      <td>male</td>
      <td>32.0</td>
      <td>0</td>
      <td>0</td>
      <td>370376</td>
      <td>7.7500</td>
      <td>NaN</td>
      <td>Q</td>
    </tr>
  </tbody>
</table>
<p>891 rows × 12 columns</p>

</div>




```python
titanic.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 891 entries, 0 to 890
    Data columns (total 12 columns):
     #   Column       Non-Null Count  Dtype  
    ---  ------       --------------  -----  
     0   PassengerId  891 non-null    int64  
     1   Survived     891 non-null    int64  
     2   Pclass       891 non-null    int64  
     3   Name         891 non-null    object 
     4   Sex          891 non-null    object 
     5   Age          714 non-null    float64
     6   SibSp        891 non-null    int64  
     7   Parch        891 non-null    int64  
     8   Ticket       891 non-null    object 
     9   Fare         891 non-null    float64
     10  Cabin        204 non-null    object 
     11  Embarked     889 non-null    object 
    dtypes: float64(2), int64(5), object(5)
    memory usage: 83.7+ KB


## 데이터 결측치 처리


```python
titanic['Age'].value_counts()
```




    24.00    30
    22.00    27
    18.00    26
    19.00    25
    30.00    25
             ..
    55.50     1
    70.50     1
    66.00     1
    23.50     1
    0.42      1
    Name: Age, Length: 88, dtype: int64




```python
titanic['Age'].unique()
```




    array([22.  , 38.  , 26.  , 35.  ,   nan, 54.  ,  2.  , 27.  , 14.  ,
            4.  , 58.  , 20.  , 39.  , 55.  , 31.  , 34.  , 15.  , 28.  ,
            8.  , 19.  , 40.  , 66.  , 42.  , 21.  , 18.  ,  3.  ,  7.  ,
           49.  , 29.  , 65.  , 28.5 ,  5.  , 11.  , 45.  , 17.  , 32.  ,
           16.  , 25.  ,  0.83, 30.  , 33.  , 23.  , 24.  , 46.  , 59.  ,
           71.  , 37.  , 47.  , 14.5 , 70.5 , 32.5 , 12.  ,  9.  , 36.5 ,
           51.  , 55.5 , 40.5 , 44.  ,  1.  , 61.  , 56.  , 50.  , 36.  ,
           45.5 , 20.5 , 62.  , 41.  , 52.  , 63.  , 23.5 ,  0.92, 43.  ,
           60.  , 10.  , 64.  , 13.  , 48.  ,  0.75, 53.  , 57.  , 80.  ,
           70.  , 24.5 ,  6.  ,  0.67, 30.5 ,  0.42, 34.5 , 74.  ])



### pandas.DataFrame.fillna

*DataFrame.fillna(value=None, method=None, axis=None, inplace=False, limit=None, downcast=None)


```python
# 결측치 채워주기
titanic['Age'].mean()
titanic['Age'].fillna(titanic['Age'].mean(),inplace = True)
titanic['Cabin'].fillna('N',inplace = True)
titanic['Fare'].fillna(titanic['Fare'].mean(),inplace = True)
titanic.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 891 entries, 0 to 890
    Data columns (total 12 columns):
     #   Column       Non-Null Count  Dtype  
    ---  ------       --------------  -----  
     0   PassengerId  891 non-null    int64  
     1   Survived     891 non-null    int64  
     2   Pclass       891 non-null    int64  
     3   Name         891 non-null    object 
     4   Sex          891 non-null    object 
     5   Age          891 non-null    float64
     6   SibSp        891 non-null    int64  
     7   Parch        891 non-null    int64  
     8   Ticket       891 non-null    object 
     9   Fare         891 non-null    float64
     10  Cabin        891 non-null    object 
     11  Embarked     889 non-null    object 
    dtypes: float64(2), int64(5), object(5)
    memory usage: 83.7+ KB



```python
# 성별, 요금, 정착항구, 선실번호 수수
print(f"남여 인원수 \n {titanic['Sex'].value_counts()}")
print(f"요금 분포 수 \n {titanic['Fare'].value_counts()}")
print(f"탑승 지역 수 \n {titanic['Embarked'].value_counts()}")
print(f"선실 번호 수 \n {titanic['Cabin'].value_counts()}")
```

    남여 인원수 
     male      577
    female    314
    Name: Sex, dtype: int64
    요금 분포 수 
     8.0500     43
    13.0000    42
    7.8958     38
    7.7500     34
    26.0000    31
               ..
    8.4583      1
    9.8375      1
    8.3625      1
    14.1083     1
    17.4000     1
    Name: Fare, Length: 248, dtype: int64
    탑승 지역 수 
     S    644
    C    168
    Q     77
    Name: Embarked, dtype: int64
    선실 번호 수 
     N              687
    G6               4
    C23 C25 C27      4
    B96 B98          4
    F2               3
                  ... 
    F38              1
    E10              1
    F G63            1
    A26              1
    C62 C64          1
    Name: Cabin, Length: 148, dtype: int64


### pandas.DataFrame.groupby

* DataFrame.groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=NoDefault.no_default, observed=False, dropna=True)


```python
titanic.groupby(['Sex','Survived'])['Survived'].count()
```




    Sex     Survived
    female  0            81
            1           233
    male    0           468
            1           109
    Name: Survived, dtype: int64




```python
import seaborn as sns
sns.barplot(x = 'Sex', y ='Survived', data = titanic)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7effb6e76f90>




![png](\github_img\titanic\output_11_1.png)
    


### seaborn.barplot

* seaborn.barplot(*, x=None, y=None, hue=None, data=None, order=None, hue_order=None, estimator=<function mean at 0x7ff320f315e0>, ci=95, n_boot=1000, units=None, seed=None, orient=None, color=None, palette=None, saturation=0.75, errcolor='.26', errwidth=None, capsize=None, dodge=True, ax=None, **kwargs)


```python
sns.barplot(x = 'Pclass', y ='Survived', hue='Sex', data = titanic)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7effb6e76290>




![png](\github_img\titanic\output_13_1.png)
    


### 생존률 예측 하기


```python
from sklearn.preprocessing import LabelEncoder
from sklearn import preprocessing

def encode_featuers(dataDF) :
    features =['Cabin', 'Sex', 'Embarked']
    for feature in features :
        le = LabelEncoder()
        le = le.fit(dataDF[feature])
        dataDF[feature] = le.transform(dataDF[feature])

    return dataDF

titanic = encode_featuers(titanic)
titanic.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }


    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }

</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>1</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>146</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>0</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>81</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>0</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>146</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>0</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>55</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>1</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>146</td>
      <td>2</td>
    </tr>
  </tbody>
</table>

</div>




```python
def fillna(df):
    df['Age' ]. filina(df ['Age'].mean, inplace=True) 
    df [ 'Cabin'].fillna('N', inplace=True) 
    df [ 'Embarked'].fillna('N', inplace=True) 
    df [ 'Fare'].fillna(0, inplace=True) 
    return df

def drop_features(df):
    df.drop( ['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True) 
    return df

def format_features(df):
    df['Cabin'] = df[ 'Cabin'].str[:1] 
    features = ['Cabin', 'Sex', 'Embarked']
    for feature in features :
        le = LabelEncoder() 
        le = le.fit(df[feature])
        df [feature] = le.transform(df[feature]) 
    return df

def transform_features(df):
    df = drop_features(df) 
    df = format_features(df) 
    return df




```


```python
filename ='/content/drive/MyDrive/muchin Learning/titanic1.csv'
titanic_df = pd.read_csv(filename)

# 결측치 채워주기
titanic_df['Age'].mean()
titanic_df['Age'].fillna(titanic_df['Age'].mean(),inplace = True)
titanic_df['Cabin'].fillna('N',inplace = True)
titanic_df['Fare'].fillna(titanic_df['Fare'].mean(),inplace = True)
titanic_df.info()

```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 891 entries, 0 to 890
    Data columns (total 12 columns):
     #   Column       Non-Null Count  Dtype  
    ---  ------       --------------  -----  
     0   PassengerId  891 non-null    int64  
     1   Survived     891 non-null    int64  
     2   Pclass       891 non-null    int64  
     3   Name         891 non-null    object 
     4   Sex          891 non-null    object 
     5   Age          891 non-null    float64
     6   SibSp        891 non-null    int64  
     7   Parch        891 non-null    int64  
     8   Ticket       891 non-null    object 
     9   Fare         891 non-null    float64
     10  Cabin        891 non-null    object 
     11  Embarked     889 non-null    object 
    dtypes: float64(2), int64(5), object(5)
    memory usage: 83.7+ KB



```python
x_titanic.columns
```




    Index(['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked'], dtype='object')




```python
y_titanic = titanic_df['Survived']
x_titanic = titanic_df.drop('Survived', axis = 1)

x_titanic = transform_features(x_titanic)
#x_titanic.drop(['SibSp','Parch'], axis= 1, inplace=True)
```


```python
x_titanic
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }


    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }

</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>1</td>
      <td>22.000000</td>
      <td>1</td>
      <td>0</td>
      <td>7.2500</td>
      <td>7</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0</td>
      <td>38.000000</td>
      <td>1</td>
      <td>0</td>
      <td>71.2833</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>0</td>
      <td>26.000000</td>
      <td>0</td>
      <td>0</td>
      <td>7.9250</td>
      <td>7</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>0</td>
      <td>35.000000</td>
      <td>1</td>
      <td>0</td>
      <td>53.1000</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>1</td>
      <td>35.000000</td>
      <td>0</td>
      <td>0</td>
      <td>8.0500</td>
      <td>7</td>
      <td>2</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>886</th>
      <td>2</td>
      <td>1</td>
      <td>27.000000</td>
      <td>0</td>
      <td>0</td>
      <td>13.0000</td>
      <td>7</td>
      <td>2</td>
    </tr>
    <tr>
      <th>887</th>
      <td>1</td>
      <td>0</td>
      <td>19.000000</td>
      <td>0</td>
      <td>0</td>
      <td>30.0000</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>888</th>
      <td>3</td>
      <td>0</td>
      <td>29.699118</td>
      <td>1</td>
      <td>2</td>
      <td>23.4500</td>
      <td>7</td>
      <td>2</td>
    </tr>
    <tr>
      <th>889</th>
      <td>1</td>
      <td>1</td>
      <td>26.000000</td>
      <td>0</td>
      <td>0</td>
      <td>30.0000</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>890</th>
      <td>3</td>
      <td>1</td>
      <td>32.000000</td>
      <td>0</td>
      <td>0</td>
      <td>7.7500</td>
      <td>7</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>891 rows × 8 columns</p>

</div>




```python
from sklearn.preprocessing import MinMaxScaler
def nomalrize_features(df) :
    ms = MinMaxScaler()
    ms.fit(df)
    df = ms.transform(df)

    return df

nomalrize_x_titanic = nomalrize_features(x_titanic)
nomalrize_x_titanic
```




    array([[1.        , 1.        , 0.27117366, ..., 0.01415106, 0.875     ,
            0.66666667],
           [0.        , 0.        , 0.4722292 , ..., 0.13913574, 0.25      ,
            0.        ],
           [1.        , 0.        , 0.32143755, ..., 0.01546857, 0.875     ,
            0.66666667],
           ...,
           [1.        , 0.        , 0.36792055, ..., 0.04577135, 0.875     ,
            0.66666667],
           [0.        , 1.        , 0.32143755, ..., 0.0585561 , 0.25      ,
            0.        ],
           [1.        , 1.        , 0.39683338, ..., 0.01512699, 0.875     ,
            0.33333333]])




```python
X_titanic_df_normalize = pd.DataFrame(nomalrize_x_titanic, columns=[ 'Pclass', 'Sex', 'Age', 'SibSp',
       'Parch', 'Fare', 'Cabin', 'Embarked'])
```


```python
X_titanic_df_normalize
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }


    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }

</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.271174</td>
      <td>0.125</td>
      <td>0.000000</td>
      <td>0.014151</td>
      <td>0.875</td>
      <td>0.666667</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.472229</td>
      <td>0.125</td>
      <td>0.000000</td>
      <td>0.139136</td>
      <td>0.250</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.321438</td>
      <td>0.000</td>
      <td>0.000000</td>
      <td>0.015469</td>
      <td>0.875</td>
      <td>0.666667</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.434531</td>
      <td>0.125</td>
      <td>0.000000</td>
      <td>0.103644</td>
      <td>0.250</td>
      <td>0.666667</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.434531</td>
      <td>0.000</td>
      <td>0.000000</td>
      <td>0.015713</td>
      <td>0.875</td>
      <td>0.666667</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>886</th>
      <td>0.5</td>
      <td>1.0</td>
      <td>0.334004</td>
      <td>0.000</td>
      <td>0.000000</td>
      <td>0.025374</td>
      <td>0.875</td>
      <td>0.666667</td>
    </tr>
    <tr>
      <th>887</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.233476</td>
      <td>0.000</td>
      <td>0.000000</td>
      <td>0.058556</td>
      <td>0.125</td>
      <td>0.666667</td>
    </tr>
    <tr>
      <th>888</th>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.367921</td>
      <td>0.125</td>
      <td>0.333333</td>
      <td>0.045771</td>
      <td>0.875</td>
      <td>0.666667</td>
    </tr>
    <tr>
      <th>889</th>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.321438</td>
      <td>0.000</td>
      <td>0.000000</td>
      <td>0.058556</td>
      <td>0.250</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>890</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.396833</td>
      <td>0.000</td>
      <td>0.000000</td>
      <td>0.015127</td>
      <td>0.875</td>
      <td>0.333333</td>
    </tr>
  </tbody>
</table>
<p>891 rows × 8 columns</p>

</div>




```python
from sklearn.model_selection import train_test_split

X_train_n, X_test_n, Y_train_n , Y_test_n = train_test_split(X_titanic_df_normalize, y_titanic, test_size=0.2, random_state=18)
```

#### 머신러닝 모델 적용 (DecisionTree,LogisticRegression, RandomForest) 


```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

def model_predict(clf, X_train, X_test, Y_train , Y_test) :
    clf.fit(X_train, Y_train)
    pred = clf.predict(X_test)
    return pred

def model_accuracy(Y_test,pred):
    score = accuracy_score(Y_test, pred)
    return score

de_clf = DecisionTreeClassifier(random_state =10)
ref_clf = RandomForestClassifier(random_state =10)
lr_clf = LogisticRegression()

clf_list = [de_clf,ref_clf, lr_clf]
score_list = []
for i in range(3):
    pred = model_predict(clf_list[i], X_train_n, X_test_n, Y_train_n , Y_test_n)
    score = model_accuracy(Y_test_n, pred)
    score_list.append(score)

print(f'DecisionTreeClassifier의 accuracy : {score_list[0]}')
print(f'RandomForestClassifier의 accuracy : {score_list[1]}')
print(f'LogisticRegression의 accuracy : {score_list[2]}')
```

    DecisionTreeClassifier의 accuracy : 0.8100558659217877
    RandomForestClassifier의 accuracy : 0.8715083798882681
    LogisticRegression의 accuracy : 0.8324022346368715



```python
X_train, X_test, Y_train , Y_test = train_test_split(x_titanic, y_titanic, test_size=0.2, random_state=18)

```


```python
score_list = []
for i in range(3):
    pred = model_predict(clf_list[i], X_train, X_test, Y_train , Y_test)
    score = model_accuracy(Y_test, pred)
    score_list.append(score)

print(f'DecisionTreeClassifier의 accuracy : {score_list[0]}')
print(f'RandomForestClassifier의 accuracy : {score_list[1]}')
print(f'LogisticRegression의 accuracy : {score_list[2]}')
```

    DecisionTreeClassifier의 accuracy : 0.8100558659217877
    RandomForestClassifier의 accuracy : 0.8715083798882681
    LogisticRegression의 accuracy : 0.8268156424581006


    /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,


#### cross_val_score 많이씀


```python
from sklearn.model_selection import cross_val_score

clf_list = [de_clf,ref_clf, lr_clf]

cross_val_list = []
for i in range(3) :
    #scorce = cross_val_score(clf_list[i], X_train, Y_train, cv = 5 )
    scorce = cross_val_score(clf_list[i], X_train_n, Y_train_n, cv = 5 )
    scorce = scorce.mean()
    cross_val_list.append(score)

cross_val_list

```




    [0.8268156424581006, 0.8268156424581006, 0.8268156424581006]

