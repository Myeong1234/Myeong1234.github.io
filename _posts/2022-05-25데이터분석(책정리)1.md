---
title: "데이터 분석 이론 정리1"
toc: true
toc_sticky: true
excerpt_separator: "<!--more-->"
author_profile: true
sidebar:
  nav: "main"
categories:
  - Post Formats
tags:

  - python
  - 데이터분석
---





# 데이터 분석 정리



## 문제 정의 및 가설 설정

문제를 정확하게 정의하고 목적을 설정하는 과정은 모든 분석 프로젝트 과정에서 가장 중요한 과정이라고 해도 과언이 아니다. 배경을 통해 문제를 정의하면 아래와 같이 정리할 수 있다. 만약 실무에서 모호한 부분이 있다면, 분석 요청자와 충분한 논의를 거쳐 이 단계를 보다 명확하게 진행하고 다음 단계로 넘어가는 것이 중요하다.

- **Funnel 방식으로 유저의 행동 패턴을 파악하여 해당 시나리오의 Bottleneck 구간을 탐색하자**
- **유저를 세분화하여 Bottleneck 개선 방안에 유용한 Insight를 도출하자**



### 가설 정립 및 예상 Output

가설을 설정하는 것은 도메인 지식이 충분하지 않거나 분석 업무 경험이 많지 않다면 어려울 수 있는 부분이다. 가설 설정은 상황에 따라 생략할 수 있는 부분이며, 오히려 데이터 탐색의 범위를 좁히는 결과를 낳기도 하기 때문에 목적 설정 과정에 비해 중요도가 높지 않다고 할 수 있다.그러나 가설을 만들고 예상 산출물(Output)을 그려보는 과정은 데이터 수집 및 분석 과정에서 수많은 의사결정을 진행할시 도움이 되기도 한다.



### 분석 프레임

보통 Frame은 최상위 목적, 그리고 그 목적을 달성하기 위한 하위 목적, 그리고 세부적인 Task로 구성이 된다. 이러한 구성을 위해 목표와 Task를 잘 구분하고 분류하는 것이 중요한데, 이 과정에 많이 쓰이는 철학과 접근법이 **MECE**이다.

MECE는 Mutually Exclusive Collectively Exhaustive의 약자로, 항목을 겹치지 않고 누락되지 않게 잘 나눈 것. 특히 비즈니스 목적이나 데이터 분석의 목적을 달성하기 위한 Frame을 구성할 때 유용하게 사용할 수 있다.



### Logic Tree

Logic Tree 모습은 마치 머신러닝 기법인 Decision Tree의 유사하다. 최종 목적을 세부적인 목적으로 분류하는 데 도움을 주는 방법론이다. 상위 단계에서 하위 단계로 구분되어 내려갈 때 MECE 철학에 근거해 잘게 쪼개질 때까지 진행하는 것이 중요하다. 

![로직트리](https://wikidocs.net/images/page/16571/logic_tree.png)





MECE와 Logic Tree를 이용해, 유저 사용성 분석의 목적에 맞는 분석 Frame을 작성/실습해보자. 아래 예제를 참고하여 파워포인트나 키노트, 혹은 메모장 등 프로그램을 실행시켜 최종 목적부터 세부적인 목적까지 직접 작성해보고 Feedback을 구해보자

### 추가 고려 요소

데이터가 입수되고 앞에서 살펴본 문제 정의, 가설 설정 및 분석 Frame 단계를 완료하였다면 분석 준비가 완료되었다고 볼 수 있다. 다만, 실무에서는 추가로 고려해야할 사항이 더 있다. 주로 Management와 관련된 부분으로, PM 확은 Team Manager, 분석 요청자와 아래와 같은 사항을 논의할 필요가 있다. 이미 언급한 대로, 분석 프로젝트는 분석가 혹은 분석팀 자체적으로 진행되는 경우가 없으므로 프로젝트를 둘러싼 외부적 상황에 따라 크게 영향 받기도 한다. 따라서 아래와 같은 고려 사항을 소홀히하지 않도록 해야 불필요한 리소스 낭비를 최소화할 수 있다.

- 일정 (중간 단계별 공유, 최종 Ouput 발표, 공유 방식 등)
- Project/Task 우선순위 (동 기간 타 프로젝트가 있을 경우, 우선순위 조정)
- 유관팀 사전 논의 및 데이터 엔지니어 지정 (Co-workers)
- 필요 Resource (소프트웨어, 서버 등)



## 데이터 전처리

목적을 정의했고 가설 정립 및 분석 Frame 단계를 마쳤다면, 실제 데이터를 수집/추출하여 분석 단계를 준비하는 **데이터 전처리** 단계를 진행한다. 수집/추출 단계는 데이터 엔지니어의 역할이 크고, 또 이 수업의 범위를 벗어난 내용이므로 생략한다. 단, 이러한 수집/추출 과정에서 무엇을 수집할지 그리고 데이터의 Quality를 파악하는 것은 분석가가 관여하는 부분임을 인지할 필요가 있다. 일반적으로 아래와 같은 이유로 바로 실제 분석을 하기 전, 데이터 전처리 단계를 반드시 거쳐야 한다.



- 여러 데이터 소스 활용: 테이블 스키마나 Label, Type 등이 상이하여, 분석을 위해 일관적인 방식으로 통합/변환 필요
- 데이터 기록/수집의 누락 및 오류 (Missing Value, Errors, typo 발생)
- 자연스러운 혹은 기계적으로 발생하는 이상치 or 극단치
- 분석 목적에 맞지 않은 변수 혹은 분포



### 데이터 전처리 가이드

위에 나열된 내용 이외에도 예상하지 못한 이유로 인해 결측치 및 이상치 등이 쉽게 발생할 수 있으며, 이러한 문제가 있는 데이터를 전처리하지 않고 분석 혹은 모델링 등의 업무를 진행할 경우 치명적인 문제/오류를 발생시킬 수 있으므로 매우 주의할 필요가 있다. 일반적으로 각 문제점별 데이터 전처리를 하는 방식은 아래와 같다.

- 데이터 Type, Label 등이 일관적이지 않은 경우
  - 프로그램에서 제공하는 함수를 통해 일괄적으로 변경 (예, SQL: Cast, Python: astype())
- Missing Value
  - 수치형인 경우 Mean, Median 등 대푯값으로 채우거나 실수 예측 모델링 활용 (예, Linear Regression)
  - 카테고리형인 경우 Mode로 채우거나 분류 예측 모델링 활용 (예, Logistic Regression)
- Errors, Typo 발생의 경우
  - 텍스트 처리 함수 활용 (예, Python: str.replace())
- 이상치(outlier)
  - IQR, Z-score, MAD 등 방식으로 이상치 제거
- 변수가 많은 경우(20개 이상)
  - PCA 등으로 차원 축소하거나 변수 중요도 파악후 불필요 변수 제거
- 편향된 분포의 변수가 존재하는 경우
  - log, sqrt 등 함수로 분포 변환
- 측정 단위(scale)이 차이가 클 경우
  - StarndardScale or MinMaxScaler 통해 스케일링



### 전처리 내용

#### 날짜를 pandas datetime 형태로 변환

```python
## 날짜가 String인 경우 
df['datetime'] = pd.to_datetime(df['datetime'])
pd.Series(pd.to_datetime(['2005/11/23', '2010.12.31', '2011.1.1']))

## 날짜가 timestamp인 경우
df['datetime'] = pd.to_datetime(df['timestamp'], unit='s').dt.date
pd.Series(pd.to_datetime([1349720105, 1349806505, 1349892905, 1349979305, 1350065705], unit='s'))#.dt.date

## 컬럼 타입을 바꾸는 경우
df1['datetime'] = df1['datetime'].astype('datetime64[ns]')
```



#### 결측치 처리

경우에 따라 결측치 처리 방법이 달라진다. 만약 샘플수가 많다면 missing values 를 포함하는 행을 모두 삭제하는 것이 가능하다.

```python
# 결측치가 하나라도 있으면 버리는 코드 예제
df.dropna()

# 모든 값이 Null일 경우만 버리는 코드 예제
df.dropna(how='all')

# 결측치가 하나 이상 있는 Case만 선택하는 코드 예제
df[df.isnull().any(axis=1)]
```

만약 샘플수가 충분하지 않을 경우, Pandas의 fillna() 명령어로 Null 값을 채우는 것이 가능하다.

- 연속형인 경우 Mean이나 Median 등 대푯값 이용
- 명목형인 경우 Mode(최빈치)나 예측 모형을 통해 Null 값 대체

경우에 따라 도메인 지식을 이용해 결측치를 처리하거나, 회귀 및 분류 예측모델을 이용하는 경우도 있다.

```python
# Null 값을 median으로 대체하는 코드 예제
df.fillna(df.mean()) 

# Null 값을 mode로 대체하는 코드 예제
freq_values = df_ms.documentposition.value_counts().index[0]
df['col1'] = df['col1'].fillna(freq_values)

# Null 값을 KNN predictive model을 이용해 처리하는 예제
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

X_train, X_test, y_train, y_test = train_test_split(df_ind, df_tar, random_state=0)

knn = KNeighborsClassifier(n_neighbors=5).fit(X_train, y_train)
fill_values = knn.predict(df[ind_cols].apply(lambda x: encoder.fit_transform(x)))
```

#### 텍스트 통일

같은 의미를 지니는 텍스트이나 다른 카테고리 값으로 수집된 경우, 해당 카테고리의 값을 동일하게 맞춰줄 필요가 있다.

- dict 형태로 기존 값과 변경 값을 저장
- replace 명령어을 통해 일괄적으로 변경

```python
# dict 생성
ext_dic = {'DOCX': 'DOC',
           'XLSX': 'XLS',
           'PPTX': 'PPT',
           'PPSX': 'PPT',
           'PPS': 'PPT',
           'ODT': 'TXT',
           'PNG': 'JPG'}

# 해당 컬럼에 replace 함수와 dict 적용
df['ext'] = df['ext'].replace(ext_dic)
```

#### 신규 id 부여

ID 와 같이 유니크 식별자의 경우, 기계적으로 부여되므로 알아보기 어려울 뿐만 아니라 데이터 사이즈를 키우는 원인이 된다. 이를 간단하게 변경해서 처리해야 할 경우가 있다.

- 빈 리스트 생성
- ID 컬럼의 행 별로 동일 여부 비교
- 만약 전행과 후행이 동일하다면 동일한 값을 부여, 아닐 경우 +1

```python
s = [] # empty list
j = 0 # default setting

# loop
for i in range(len(df)-1):

    # compare each rows
    if df.ix[i, 'sessionid'] == df.ix[i+1, 'sessionid']:
        s.append(j)

    # update j values
    else:
        s.append(j)
        j += 1
```

#### 대소문자 처리

필수는 아니지만, 모든 텍스트가 소문자일 경우 대소문자 여부가 중요하지 않게 된다.

- 타이핑시 대소문자 변환 불필요
- 오탈자 문제 방지
- 생산성 증대

```python
cols = df.columns # list of columns' names

# loop
for c in cols:
    if c != 'datetime':  
        df[c] = df[c].apply(lambda x: x.lower())
```



















