---
title: "뉴스기사 분류"
toc: true
toc_sticky: true
excerpt_separator: "<!--more-->"
author_profile: true
sidebar:
  nav: "main"
categories:
  - Post Formats
tags:
   - python
   - 데이터 분석
---



# 뉴스 기사 분류 : 다중 분류 문제

## 로이터 데이터셋 : 46개 토픽


```python
from keras.datasets import reuters
import pandas as pd
import numpy as np


```


```python
(train_data , train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)
```


```python
train_data
```




    array([list([1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]),
           list([1, 3267, 699, 3434, 2295, 56, 2, 7511, 9, 56, 3906, 1073, 81, 5, 1198, 57, 366, 737, 132, 20, 4093, 7, 2, 49, 2295, 2, 1037, 3267, 699, 3434, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2295, 2, 2, 775, 7, 48, 34, 191, 44, 35, 1795, 505, 17, 12]),
           list([1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32, 818, 15, 14, 272, 26, 39, 684, 70, 11, 14, 12, 3886, 18, 180, 183, 187, 70, 11, 14, 102, 32, 11, 29, 53, 44, 704, 15, 14, 19, 758, 15, 53, 959, 47, 1013, 15, 14, 19, 132, 15, 39, 965, 32, 11, 14, 147, 72, 11, 180, 183, 187, 44, 11, 14, 102, 19, 11, 123, 186, 90, 67, 960, 4, 78, 13, 68, 467, 511, 110, 59, 89, 90, 67, 1390, 55, 2678, 92, 617, 80, 1274, 46, 905, 220, 13, 4, 346, 48, 235, 629, 5, 211, 5, 1118, 7, 2, 81, 5, 187, 11, 15, 9, 1709, 201, 5, 47, 3615, 18, 478, 4514, 5, 1118, 7, 232, 2, 71, 5, 160, 63, 11, 9, 2, 81, 5, 102, 59, 11, 17, 12]),
           ...,
           list([1, 141, 3890, 387, 81, 8, 16, 1629, 10, 340, 1241, 850, 31, 56, 3890, 691, 9, 1241, 71, 9, 5985, 2, 2, 699, 2, 2, 2, 699, 244, 5945, 4, 49, 8, 4, 656, 850, 33, 2993, 9, 2139, 340, 3371, 1493, 9, 2, 22, 2, 1094, 687, 83, 35, 15, 257, 6, 57, 9190, 7, 4, 5956, 654, 5, 2, 6191, 1371, 4, 49, 8, 16, 369, 646, 6, 1076, 7, 124, 407, 17, 12]),
           list([1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, 258, 3614, 18, 14, 74, 134, 5131, 18, 88, 2321, 72, 11, 14, 1842, 32, 11, 123, 383, 89, 39, 46, 235, 10, 864, 728, 5, 258, 44, 11, 15, 22, 753, 9, 42, 92, 131, 728, 5, 69, 312, 11, 15, 22, 222, 2, 3237, 383, 48, 39, 74, 235, 10, 864, 276, 5, 61, 32, 11, 15, 21, 4, 211, 5, 126, 1072, 42, 92, 131, 46, 19, 352, 11, 15, 22, 710, 220, 9, 42, 92, 131, 276, 5, 59, 61, 11, 15, 22, 10, 455, 7, 1172, 137, 336, 1325, 6, 1532, 142, 971, 6463, 43, 359, 5, 4, 326, 753, 364, 17, 12]),
           list([1, 227, 2406, 91, 2, 125, 2855, 21, 4, 3976, 76, 7, 4, 757, 481, 3976, 790, 5259, 5654, 9, 111, 149, 8, 7, 10, 76, 223, 51, 4, 417, 8, 1047, 91, 6917, 1688, 340, 7, 194, 9411, 6, 1894, 21, 127, 2151, 2394, 1456, 6, 3034, 4, 329, 433, 7, 65, 87, 1127, 10, 8219, 1475, 290, 9, 21, 567, 16, 1926, 24, 4, 76, 209, 30, 4033, 6655, 5654, 8, 4, 60, 8, 4, 966, 308, 40, 2575, 129, 2, 295, 277, 1071, 9, 24, 286, 2114, 234, 222, 9, 4, 906, 3994, 8519, 114, 5758, 1752, 7, 4, 113, 17, 12])],
          dtype=object)




```python
train_labels
```




    array([ 3,  4,  3, ..., 25,  3, 25], dtype=int64)




```python
test_data
```




    array([list([1, 4, 1378, 2025, 9, 697, 4622, 111, 8, 25, 109, 29, 3650, 11, 150, 244, 364, 33, 30, 30, 1398, 333, 6, 2, 159, 9, 1084, 363, 13, 2, 71, 9, 2, 71, 117, 4, 225, 78, 206, 10, 9, 1214, 8, 4, 270, 5, 2, 7, 748, 48, 9, 2, 7, 207, 1451, 966, 1864, 793, 97, 133, 336, 7, 4, 493, 98, 273, 104, 284, 25, 39, 338, 22, 905, 220, 3465, 644, 59, 20, 6, 119, 61, 11, 15, 58, 579, 26, 10, 67, 7, 4, 738, 98, 43, 88, 333, 722, 12, 20, 6, 19, 746, 35, 15, 10, 9, 1214, 855, 129, 783, 21, 4, 2280, 244, 364, 51, 16, 299, 452, 16, 515, 4, 99, 29, 5, 4, 364, 281, 48, 10, 9, 1214, 23, 644, 47, 20, 324, 27, 56, 2, 2, 5, 192, 510, 17, 12]),
           list([1, 2768, 283, 122, 7, 4, 89, 544, 463, 29, 798, 748, 40, 85, 306, 28, 19, 59, 11, 82, 84, 22, 10, 1315, 19, 12, 11, 82, 52, 29, 283, 1135, 558, 2, 265, 2, 6607, 8, 6607, 118, 371, 10, 1503, 281, 4, 143, 4811, 760, 50, 2088, 225, 139, 683, 4, 48, 193, 862, 41, 967, 1999, 30, 1086, 36, 8, 28, 602, 19, 32, 11, 82, 5, 4, 89, 544, 463, 41, 30, 6273, 13, 260, 951, 6607, 8, 69, 1749, 18, 82, 41, 30, 306, 3342, 13, 4, 37, 38, 283, 555, 649, 18, 82, 13, 1721, 282, 9, 132, 18, 82, 41, 30, 385, 21, 4, 169, 76, 36, 8, 107, 4, 106, 524, 10, 295, 3825, 2, 2476, 6, 3684, 6940, 4, 1126, 41, 263, 84, 395, 649, 18, 82, 838, 1317, 4, 572, 4, 106, 13, 25, 595, 2445, 40, 85, 7369, 518, 5, 4, 1126, 51, 115, 680, 16, 6, 719, 250, 27, 429, 6607, 8, 6940, 114, 343, 84, 142, 20, 5, 1145, 1538, 4, 65, 494, 474, 27, 69, 445, 11, 1816, 6607, 8, 109, 181, 2768, 2, 62, 1810, 6, 624, 901, 6940, 107, 4, 1126, 34, 524, 4, 6940, 1126, 41, 447, 7, 1427, 13, 69, 251, 18, 872, 876, 1539, 468, 9063, 242, 5, 646, 27, 1888, 169, 283, 87, 9, 10, 2, 260, 182, 122, 678, 306, 13, 4, 99, 216, 7, 89, 544, 64, 85, 2333, 6, 195, 7254, 6337, 268, 609, 4, 195, 41, 1017, 2765, 2, 4, 73, 706, 2, 92, 4, 91, 3917, 36, 8, 51, 144, 23, 1858, 129, 564, 13, 269, 678, 115, 55, 866, 189, 814, 604, 838, 117, 380, 595, 951, 320, 4, 398, 57, 2233, 7411, 269, 274, 87, 6607, 8, 787, 283, 34, 596, 661, 5467, 13, 2362, 1816, 90, 2, 84, 22, 2202, 1816, 54, 748, 6607, 8, 87, 62, 6154, 84, 161, 5, 1208, 480, 4, 2, 416, 6, 538, 122, 115, 55, 129, 1104, 1445, 345, 389, 31, 4, 169, 76, 36, 8, 787, 398, 7, 4, 2, 1507, 64, 8862, 22, 125, 2, 9, 2876, 172, 399, 9, 2, 5206, 9, 2, 122, 36, 8, 6642, 172, 247, 100, 97, 6940, 34, 75, 477, 541, 4, 283, 182, 4, 2, 295, 301, 2, 125, 2, 6607, 8, 77, 57, 445, 283, 1998, 217, 31, 380, 704, 51, 77, 2, 509, 5, 476, 9, 2876, 122, 115, 853, 6, 1061, 52, 10, 2, 2, 1308, 5, 4, 283, 182, 36, 8, 5296, 114, 30, 531, 6, 6376, 9, 2470, 529, 13, 2, 2, 58, 529, 7, 2148, 2, 185, 1028, 240, 5296, 1028, 949, 657, 57, 6, 1046, 283, 36, 8, 6607, 8, 4, 2217, 34, 9177, 13, 10, 4910, 5, 4, 141, 283, 120, 50, 2877, 7, 1049, 43, 10, 181, 283, 734, 115, 55, 3356, 476, 6, 2195, 10, 73, 120, 50, 41, 6877, 169, 87, 6607, 8, 107, 144, 23, 129, 120, 169, 87, 33, 2409, 30, 1888, 1171, 161, 4, 294, 517, 23, 2, 25, 398, 9, 2060, 283, 21, 4, 236, 36, 8, 143, 169, 87, 641, 1569, 28, 69, 61, 376, 514, 90, 1249, 62, 2, 13, 4, 2217, 696, 122, 404, 2936, 22, 134, 6, 187, 514, 10, 1249, 107, 4, 96, 1043, 1569, 13, 10, 184, 28, 61, 376, 514, 268, 680, 4, 320, 6, 154, 6, 69, 160, 514, 10, 1249, 27, 4, 153, 5, 52, 29, 36, 8, 6607, 8, 612, 408, 10, 3133, 283, 76, 27, 1504, 31, 169, 951, 2, 122, 36, 8, 283, 236, 62, 641, 84, 618, 2, 22, 8417, 8409, 9, 274, 7322, 399, 7587, 51, 115, 55, 45, 4044, 31, 4, 490, 558, 36, 8, 224, 2, 115, 57, 85, 1655, 2671, 5, 283, 6, 4, 37, 38, 7, 1797, 185, 77, 4446, 4, 555, 298, 77, 240, 2, 7, 327, 652, 194, 8773, 6233, 34, 2, 5463, 4884, 1297, 6, 240, 260, 458, 87, 6, 134, 514, 10, 1249, 22, 196, 514, 4, 37, 38, 309, 213, 54, 207, 8577, 25, 134, 139, 89, 283, 494, 555, 22, 4, 2217, 6, 2172, 4278, 434, 835, 22, 3598, 3746, 434, 835, 7, 48, 6607, 8, 618, 225, 586, 333, 122, 572, 126, 2768, 1998, 62, 133, 6, 2458, 233, 28, 602, 188, 5, 4, 704, 1998, 62, 45, 885, 281, 4, 48, 193, 760, 36, 8, 115, 680, 78, 58, 109, 95, 6, 1732, 1516, 281, 4, 225, 760, 17, 12]),
           list([1, 4, 309, 2276, 4759, 5, 2015, 403, 1920, 33, 1575, 1627, 1173, 87, 13, 536, 78, 6490, 399, 7, 2068, 212, 10, 634, 179, 8, 137, 5602, 7, 2775, 33, 30, 1015, 43, 33, 5602, 50, 489, 4, 403, 6, 96, 399, 7, 1953, 3587, 8427, 6603, 4132, 3669, 8180, 7163, 9, 2015, 8, 2, 2, 1683, 791, 5, 740, 220, 707, 13, 4, 634, 634, 54, 1405, 6331, 4, 361, 182, 24, 511, 972, 137, 403, 1920, 529, 6, 96, 3711, 399, 41, 30, 2776, 21, 10, 8491, 2002, 503, 5, 188, 6, 353, 26, 2474, 21, 432, 4, 4234, 23, 3288, 435, 34, 737, 6, 246, 7528, 274, 1173, 1627, 87, 13, 399, 992, 27, 274, 403, 87, 2631, 85, 480, 52, 2015, 403, 820, 13, 10, 139, 9, 115, 949, 609, 890, 819, 6, 812, 593, 7, 576, 7, 194, 2329, 216, 2, 8, 2, 8, 634, 33, 768, 2085, 593, 4, 403, 1920, 185, 9, 107, 403, 87, 2, 107, 1635, 410, 4, 682, 189, 161, 1635, 762, 274, 5319, 115, 30, 43, 389, 410, 4, 682, 107, 1635, 762, 456, 36, 8, 184, 4057, 95, 1854, 107, 403, 87, 302, 2, 8, 129, 100, 756, 7, 3288, 96, 298, 55, 370, 731, 866, 189, 115, 949, 9695, 115, 949, 343, 756, 2, 9, 115, 949, 343, 756, 2509, 36, 8, 17, 12]),
           ...,
           list([1, 1809, 124, 53, 653, 26, 39, 5439, 18, 14, 5893, 18, 155, 177, 53, 544, 26, 39, 19, 5121, 18, 14, 19, 6382, 18, 280, 3882, 11, 14, 3123, 32, 11, 695, 3614, 47, 11, 14, 3615, 63, 11, 430, 3259, 44, 11, 14, 61, 11, 17, 12]),
           list([1, 5586, 2, 71, 8, 23, 166, 344, 10, 78, 13, 68, 80, 467, 606, 6, 261, 5, 146, 93, 124, 4, 166, 75, 3603, 2, 5907, 265, 8692, 1251, 2, 297, 1127, 195, 9, 621, 575, 1080, 5907, 7, 378, 104, 421, 648, 20, 5, 4, 49, 2, 8, 1708, 28, 4, 303, 163, 524, 10, 1220, 6, 455, 4, 326, 685, 6, 2, 422, 71, 142, 73, 863, 62, 75, 3603, 6, 4, 326, 166, 2, 34, 1652, 3603, 6, 4, 166, 4, 49, 8, 17, 12]),
           list([1, 706, 209, 658, 4, 37, 38, 309, 484, 4, 1434, 6, 933, 4, 89, 709, 377, 101, 28, 4, 143, 511, 101, 5, 47, 758, 15, 90, 2388, 7, 809, 6, 444, 2035, 4, 911, 5, 709, 198, 1997, 634, 3644, 3798, 2305, 8, 1486, 6, 674, 480, 10, 990, 309, 4008, 2190, 2305, 1849, 24, 68, 583, 242, 5, 4, 143, 709, 364, 7376, 41, 30, 13, 706, 6, 837, 4, 377, 101, 6, 631, 28, 47, 758, 15, 36, 1413, 107, 4, 377, 101, 62, 47, 758, 15, 634, 114, 713, 888, 1412, 6, 343, 37, 38, 1116, 95, 1136, 269, 43, 1488, 1170, 6, 226, 2, 4, 377, 101, 136, 143, 1032, 4, 89, 709, 377, 101, 1217, 30, 478, 97, 47, 948, 15, 90, 4594, 2, 5853, 41, 30, 13, 706, 6, 455, 4, 465, 474, 6, 837, 634, 6, 2069, 4, 709, 377, 101, 28, 47, 758, 15, 7, 463, 29, 89, 1017, 97, 148, 16, 6, 47, 948, 15, 4, 48, 511, 377, 101, 23, 47, 758, 15, 161, 5, 4, 47, 12, 20, 7424, 7978, 386, 240, 2305, 2634, 24, 10, 181, 1475, 7, 194, 534, 21, 709, 364, 756, 33, 30, 4, 386, 404, 36, 118, 4, 2190, 24, 4, 911, 7, 1116, 23, 24, 4, 37, 38, 377, 101, 1976, 42, 9964, 6, 127, 122, 9, 7609, 1136, 692, 13, 37, 38, 1116, 446, 69, 4, 234, 709, 7614, 1320, 13, 126, 1006, 5, 338, 458, 2305, 8, 4, 1136, 911, 23, 4, 307, 2016, 36, 8, 634, 23, 325, 2863, 4, 820, 9, 129, 2767, 40, 836, 85, 1523, 17, 12])],
          dtype=object)




```python
test_labels
```




    array([ 3, 10,  1, ...,  3,  3, 24], dtype=int64)




```python
train_data.shape, train_labels.shape
```




    ((8982,), (8982,))




```python
print(len(train_data) ,len(train_labels))
```

    8982 8982



```python
train_data_df = pd.DataFrame(train_data)
train_labels_df = pd.DataFrame(train_labels)

reuters_df = pd.DataFrame(train_data_df)
reuters_df.columns=['train_data']
reuters_df['train_labels'] = train_labels_df


reuters_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>train_data</th>
      <th>train_labels</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, ...</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>[1, 3267, 699, 3434, 2295, 56, 2, 7511, 9, 56,...</td>
      <td>4</td>
    </tr>
    <tr>
      <th>2</th>
      <td>[1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32,...</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>[1, 4, 686, 867, 558, 4, 37, 38, 309, 2276, 46...</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>[1, 8295, 111, 8, 25, 166, 40, 638, 10, 436, 2...</td>
      <td>4</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>8977</th>
      <td>[1, 313, 262, 2529, 1426, 8, 130, 40, 129, 363...</td>
      <td>19</td>
    </tr>
    <tr>
      <th>8978</th>
      <td>[1, 4, 96, 5, 340, 3976, 23, 328, 6, 154, 7, 4...</td>
      <td>19</td>
    </tr>
    <tr>
      <th>8979</th>
      <td>[1, 141, 3890, 387, 81, 8, 16, 1629, 10, 340, ...</td>
      <td>25</td>
    </tr>
    <tr>
      <th>8980</th>
      <td>[1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, ...</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8981</th>
      <td>[1, 227, 2406, 91, 2, 125, 2855, 21, 4, 3976, ...</td>
      <td>25</td>
    </tr>
  </tbody>
</table>
<p>8982 rows × 2 columns</p>
</div>




```python
reuters_df.shape
```




    (8982, 2)




```python
reuters_df.info
```




    <bound method DataFrame.info of                                              train_data  train_labels
    0     [1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, ...             3
    1     [1, 3267, 699, 3434, 2295, 56, 2, 7511, 9, 56,...             4
    2     [1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32,...             3
    3     [1, 4, 686, 867, 558, 4, 37, 38, 309, 2276, 46...             4
    4     [1, 8295, 111, 8, 25, 166, 40, 638, 10, 436, 2...             4
    ...                                                 ...           ...
    8977  [1, 313, 262, 2529, 1426, 8, 130, 40, 129, 363...            19
    8978  [1, 4, 96, 5, 340, 3976, 23, 328, 6, 154, 7, 4...            19
    8979  [1, 141, 3890, 387, 81, 8, 16, 1629, 10, 340, ...            25
    8980  [1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, ...             3
    8981  [1, 227, 2406, 91, 2, 125, 2855, 21, 4, 3976, ...            25
    
    [8982 rows x 2 columns]>




```python
reuters_df.isnull().sum()
```




    train_data      0
    train_labels    0
    dtype: int64




```python
reuters_df.iloc[:,0]
```




    0       [1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, ...
    1       [1, 3267, 699, 3434, 2295, 56, 2, 7511, 9, 56,...
    2       [1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32,...
    3       [1, 4, 686, 867, 558, 4, 37, 38, 309, 2276, 46...
    4       [1, 8295, 111, 8, 25, 166, 40, 638, 10, 436, 2...
                                  ...                        
    8977    [1, 313, 262, 2529, 1426, 8, 130, 40, 129, 363...
    8978    [1, 4, 96, 5, 340, 3976, 23, 328, 6, 154, 7, 4...
    8979    [1, 141, 3890, 387, 81, 8, 16, 1629, 10, 340, ...
    8980    [1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, ...
    8981    [1, 227, 2406, 91, 2, 125, 2855, 21, 4, 3976, ...
    Name: train_data, Length: 8982, dtype: object




```python
reuters_df.loc[:]['train_data']
```




    0       [1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, ...
    1       [1, 3267, 699, 3434, 2295, 56, 2, 7511, 9, 56,...
    2       [1, 53, 12, 284, 15, 14, 272, 26, 53, 959, 32,...
    3       [1, 4, 686, 867, 558, 4, 37, 38, 309, 2276, 46...
    4       [1, 8295, 111, 8, 25, 166, 40, 638, 10, 436, 2...
                                  ...                        
    8977    [1, 313, 262, 2529, 1426, 8, 130, 40, 129, 363...
    8978    [1, 4, 96, 5, 340, 3976, 23, 328, 6, 154, 7, 4...
    8979    [1, 141, 3890, 387, 81, 8, 16, 1629, 10, 340, ...
    8980    [1, 53, 46, 957, 26, 14, 74, 132, 26, 39, 46, ...
    8981    [1, 227, 2406, 91, 2, 125, 2855, 21, 4, 3976, ...
    Name: train_data, Length: 8982, dtype: object




```python
max([max(sequence) for sequence in train_data])
```




    9999




```python
len([max(sequence) for sequence in train_data])
```




    8982




```python
word_index = reuters.get_word_index()
reverse_word_newswire = dict([(value , key) for (key , value) in word_index.items()])

# 리뷰를 디코딩합니다. 0, 1, 2는 ‘패딩’, ‘문서 시작’, ‘사전에 없음’을 위한 인덱스이므로 3을 뺍니다.
decoded_review = ' '.join([reverse_word_newswire.get(i - 3, '?') for i in train_data[0]])
```


```python
def vectorize_sequences(sequences, dimension=10000):
    # 크기가 (len(sequences), dimension)이고 모든 원소가 0인 행렬을 만듭니다.
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1. # results[i]에서 특정 인덱스의 위치를 1로 만듭니다.
    return results

x_train = vectorize_sequences(train_data) # 훈련 데이터를 벡터로 변환합니다.
x_test = vectorize_sequences(test_data) # 테스트 데이터를 벡터로 변환합니다.
```

#label Encording1
def to_one_hot(labels, dimension=46):
    # 크기가 (len(sequences), dimension)이고 모든 원소가 0인 행렬을 만듭니다.
    results = np.zeros((len(labels), dimension))
    for i, label in enumerate(labels):
        results[i, label] = 1. # results[i]에서 특정 인덱스의 위치를 1로 만듭니다.
    return results

one_hot_train_labels = to_one_hot(train_labels) # 훈련 데이터를 벡터로 변환합니다.
one_hot_test_labels = to_one_hot(test_labels) # 테스트 데이터를 벡터로 변환합니다.


```python
#label Encording2
from keras.utils.np_utils import to_categorical

one_hot_train_labels = to_categorical(train_labels) # 훈련 데이터를 벡터로 변환합니다.
one_hot_test_labels = to_categorical(test_labels) # 테스트 데이터를 벡터로 변환합니다.
```


```python
from keras import models
from keras import layers
```


```python
model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))
```


```python
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

```

## 훈련검증


```python
x_val = x_train[:1000].astype(float)
partial_x_train = x_train[1000:]

y_val = one_hot_train_labels[:1000].astype(float)
partial_y_train = one_hot_train_labels[1000:]

```


```python
history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))

```

    Epoch 1/20
    16/16 [==============================] - 2s 35ms/step - loss: 2.5821 - accuracy: 0.5649 - val_loss: 1.6754 - val_accuracy: 0.6550
    Epoch 2/20
    16/16 [==============================] - 0s 25ms/step - loss: 1.3688 - accuracy: 0.7180 - val_loss: 1.2505 - val_accuracy: 0.7310
    Epoch 3/20
    16/16 [==============================] - 0s 24ms/step - loss: 1.0087 - accuracy: 0.7804 - val_loss: 1.1024 - val_accuracy: 0.7580
    Epoch 4/20
    16/16 [==============================] - 0s 23ms/step - loss: 0.7938 - accuracy: 0.8282 - val_loss: 1.0183 - val_accuracy: 0.7810
    Epoch 5/20
    16/16 [==============================] - 0s 25ms/step - loss: 0.6340 - accuracy: 0.8639 - val_loss: 0.9547 - val_accuracy: 0.7970
    Epoch 6/20
    16/16 [==============================] - 0s 25ms/step - loss: 0.5034 - accuracy: 0.8961 - val_loss: 0.9069 - val_accuracy: 0.8090
    Epoch 7/20
    16/16 [==============================] - 0s 24ms/step - loss: 0.4057 - accuracy: 0.9152 - val_loss: 0.8981 - val_accuracy: 0.8080
    Epoch 8/20
    16/16 [==============================] - 0s 23ms/step - loss: 0.3277 - accuracy: 0.9312 - val_loss: 0.8820 - val_accuracy: 0.8090
    Epoch 9/20
    16/16 [==============================] - 0s 23ms/step - loss: 0.2685 - accuracy: 0.9400 - val_loss: 0.9047 - val_accuracy: 0.8080
    Epoch 10/20
    16/16 [==============================] - 0s 24ms/step - loss: 0.2303 - accuracy: 0.9471 - val_loss: 0.8961 - val_accuracy: 0.8130
    Epoch 11/20
    16/16 [==============================] - 0s 25ms/step - loss: 0.1988 - accuracy: 0.9499 - val_loss: 0.9320 - val_accuracy: 0.8100
    Epoch 12/20
    16/16 [==============================] - 0s 25ms/step - loss: 0.1792 - accuracy: 0.9533 - val_loss: 0.9595 - val_accuracy: 0.8020
    Epoch 13/20
    16/16 [==============================] - 0s 24ms/step - loss: 0.1589 - accuracy: 0.9549 - val_loss: 0.9614 - val_accuracy: 0.8040
    Epoch 14/20
    16/16 [==============================] - 0s 27ms/step - loss: 0.1495 - accuracy: 0.9545 - val_loss: 0.9493 - val_accuracy: 0.8150
    Epoch 15/20
    16/16 [==============================] - 0s 26ms/step - loss: 0.1360 - accuracy: 0.9549 - val_loss: 0.9438 - val_accuracy: 0.8170
    Epoch 16/20
    16/16 [==============================] - 0s 26ms/step - loss: 0.1278 - accuracy: 0.9584 - val_loss: 1.0060 - val_accuracy: 0.8090
    Epoch 17/20
    16/16 [==============================] - 0s 25ms/step - loss: 0.1219 - accuracy: 0.9579 - val_loss: 1.0300 - val_accuracy: 0.8100
    Epoch 18/20
    16/16 [==============================] - 0s 24ms/step - loss: 0.1209 - accuracy: 0.9579 - val_loss: 1.0201 - val_accuracy: 0.8110
    Epoch 19/20
    16/16 [==============================] - 0s 26ms/step - loss: 0.1129 - accuracy: 0.9562 - val_loss: 1.0815 - val_accuracy: 0.8010
    Epoch 20/20
    16/16 [==============================] - 0s 26ms/step - loss: 0.1148 - accuracy: 0.9582 - val_loss: 1.0932 - val_accuracy: 0.7980



```python
import matplotlib.pyplot as plt
```


```python
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(loss) + 1)

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

```


​    
![png](\github_img\news\output_28_0.png)
​    



```python
plt.clf()   # 그래프를 초기화합니다

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

plt.plot(epochs, acc, 'bo', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

```


![png](\github_img\news\output_29_0.png)
    



```python
model.predict(x_test)
```




    array([[6.26574263e-07, 1.09503226e-07, 7.81704812e-10, ...,
            1.41018930e-09, 4.30257739e-12, 6.13413320e-08],
           [6.43835810e-05, 1.53234571e-01, 6.66276246e-05, ...,
            4.58824765e-08, 1.99367423e-06, 1.18543700e-04],
           [1.07261694e-04, 9.82934177e-01, 1.87021610e-03, ...,
            2.91491742e-05, 4.71744187e-07, 6.35191554e-06],
           ...,
           [3.90496689e-06, 1.83063748e-05, 8.99548525e-08, ...,
            2.31948594e-08, 3.41965012e-09, 7.60024150e-08],
           [3.99547117e-03, 4.78175223e-01, 2.39927249e-04, ...,
            1.69465657e-05, 1.16693911e-06, 8.31814032e-05],
           [5.02841067e-06, 9.83672798e-01, 6.87729102e-04, ...,
            3.32993409e-06, 6.61950423e-08, 1.43894465e-06]], dtype=float32)




```python
model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))

model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
model.fit(partial_x_train,
          partial_y_train,
          epochs=9,
          batch_size=512,
          validation_data=(x_val, y_val))
results = model.evaluate(x_test, one_hot_test_labels)

```

    Epoch 1/9
    16/16 [==============================] - 1s 34ms/step - loss: 2.7429 - accuracy: 0.5164 - val_loss: 1.8020 - val_accuracy: 0.6400
    Epoch 2/9
    16/16 [==============================] - 0s 24ms/step - loss: 1.4423 - accuracy: 0.7083 - val_loss: 1.3236 - val_accuracy: 0.7160
    Epoch 3/9
    16/16 [==============================] - 0s 24ms/step - loss: 1.0561 - accuracy: 0.7766 - val_loss: 1.1402 - val_accuracy: 0.7600
    Epoch 4/9
    16/16 [==============================] - 0s 25ms/step - loss: 0.8265 - accuracy: 0.8275 - val_loss: 1.0407 - val_accuracy: 0.7870
    Epoch 5/9
    16/16 [==============================] - 0s 23ms/step - loss: 0.6593 - accuracy: 0.8624 - val_loss: 0.9676 - val_accuracy: 0.7920
    Epoch 6/9
    16/16 [==============================] - 0s 23ms/step - loss: 0.5246 - accuracy: 0.8919 - val_loss: 0.9426 - val_accuracy: 0.7930
    Epoch 7/9
    16/16 [==============================] - 0s 24ms/step - loss: 0.4209 - accuracy: 0.9144 - val_loss: 0.9134 - val_accuracy: 0.8140
    Epoch 8/9
    16/16 [==============================] - 0s 24ms/step - loss: 0.3452 - accuracy: 0.9283 - val_loss: 0.9012 - val_accuracy: 0.8170
    Epoch 9/9
    16/16 [==============================] - 0s 27ms/step - loss: 0.2866 - accuracy: 0.9371 - val_loss: 0.9140 - val_accuracy: 0.8270
    71/71 [==============================] - 2s 3ms/step - loss: 1.0060 - accuracy: 0.7885



```python
results
```




    [1.0060230493545532, 0.7885128855705261]




```python
predictions = model.predict(x_test)
```


```python
predictions[0].shape
```




    (46,)




```python
np.sum(predictions[0])
```




    0.9999999




```python
#몇번쨰 예측
np.argmax(predictions[0])
```




    3


