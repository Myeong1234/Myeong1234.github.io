---
title: "타이타닉 생존율 예측2(스코어, Threshold)"
excerpt_separator: "<!--more-->"
categories:
  - Post Formats
tags:
   - python
   - 데이터 분석
---



# 타이타닉 생존율 분석(스코어,  Threshold)


```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix


def get_clf_eval(y_test, pred):
    confusion = confusion_matrix(y_test, pred)
    accuracy = accuracy_score(y_test, pred)
    precision = precision_score(y_test, pred)
    recall = recall_score(y_test, pred)
    print("오차 행렬")
    print(confusion)
    print(f' 정확도 : {accuracy} 정밀도 : {precision} 재현율 : {recall}')

```


```python
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

from sklearn.preprocessing import LabelEncoder

def encode_features(dataFrame):
    features = ['Cabin', 'Sex', 'Embarked']
    for feature in features:
        le = LabelEncoder()
        le.fit(dataFrame[feature])
        dataFrame[feature] = le.transform(dataFrame[feature])
    return dataFrame


#결측치 처리

def fillna(df):
    df['Age'].fillna(df['Age'].mean(), inplace=True)
    df['Cabin'].fillna('N', inplace=True)
    df['Embarked'].fillna('N', inplace=True)
    df['Fare'].fillna(0, inplace=True)
    
    return df

# 불필요한 feature 제거
def drop_features(df):
    df.drop(['PassengerId' , 'Name','Ticket' ] , inplace=True, axis=1)
    
    return df

#만든 함수 실행
def transform_features(df):
    df = fillna(df)
    df = drop_features(df)
    df = encode_features(df)
    
    return df
```


```python
 
filename ='/content/drive/MyDrive/muchin Learning/titanic_2.csv'
titanic_df = pd.read_csv(filename)
y_titanic_df = titanic_df['Survived']
X_titanic_df = titanic_df.drop('Survived' , axis=1)

X_titanic_df = transform_features(X_titanic_df)
X_titanic_df.drop(['WikiId', 'Name_wiki', 'Age_wiki', 'Hometown', 'Boarded', 'Destination',
       'Lifeboat', 'Body', 'Class'],axis=1, inplace=True)
```


```python
X_train, X_test, y_train, y_test = train_test_split(X_titanic_df,y_titanic_df, test_size=0.2 , random_state=10)
```


```python
from sklearn.ensemble import RandomForestClassifier

lr_clf = LogisticRegression()
rf_clf = RandomForestClassifier(random_state=10)
lr_clf.fit(X_train,y_train)
rf_clf.fit(X_train,y_train)

pred = lr_clf.predict(X_test)
rf_pred = rf_clf.predict(X_test)

get_clf_eval(y_test, pred)
get_clf_eval(y_test, rf_pred)
```

    오차 행렬
    [[104  13]
     [ 20  42]]
     정확도 : 0.8156424581005587 정밀도 : 0.7636363636363637 재현율 : 0.6774193548387096
    오차 행렬
    [[103  14]
     [ 14  48]]
     정확도 : 0.8435754189944135 정밀도 : 0.7741935483870968 재현율 : 0.7741935483870968


    /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
    
    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,


## Threshold


```python
pred_proba = lr_clf.predict_proba(X_test)
rf_pred_proba = rf_clf.predict_proba(X_test)
pred = lr_clf.predict(X_test)
rf_pred = rf_clf.predict(X_test)

print(f'pred_proba()결과 shape : {pred_proba.shape}')
print(f'rf_pred_proba()결과 shape : {rf_pred_proba.shape}')
print('pred_proba array에서 앞 3개만 샘플로 추출 {}',pred_proba[:3])
print('rf_pred_proba array에서 앞 3개만 샘플로 추출 {}',rf_pred_proba[:3])

pred_proba_result = np.concatenate([pred_proba, pred.reshape(-1,1)],axis =1)
rf_pred_proba_result = np.concatenate([pred_proba, rf_pred.reshape(-1,1)],axis =1)
print('두개의 class 중에서 더 큰 확률을 클래스 값으로 예측 \n', pred_proba_result[:3])
print('두개의 rf class 중에서 더 큰 확률을 클래스 값으로 예측 \n', rf_pred_proba_result[:3])
```

    pred_proba()결과 shape : (179, 2)
    rf_pred_proba()결과 shape : (179, 2)
    pred_proba array에서 앞 3개만 샘플로 추출 {} [[0.86651869 0.13348131]
     [0.82652872 0.17347128]
     [0.84277059 0.15722941]]
    rf_pred_proba array에서 앞 3개만 샘플로 추출 {} [[0.98 0.02]
     [0.96 0.04]
     [0.99 0.01]]
    두개의 class 중에서 더 큰 확률을 클래스 값으로 예측 
     [[0.86651869 0.13348131 0.        ]
     [0.82652872 0.17347128 0.        ]
     [0.84277059 0.15722941 0.        ]]
    두개의 rf class 중에서 더 큰 확률을 클래스 값으로 예측 
     [[0.86651869 0.13348131 0.        ]
     [0.82652872 0.17347128 0.        ]
     [0.84277059 0.15722941 0.        ]]



```python
from sklearn.preprocessing import Binarizer

custom_threshold = 0.5

pred_proba_1 = pred_proba[:,1].reshape(-1,1)
binarizer = Binarizer(threshold = custom_threshold).fit(pred_proba_1)
custom_predict = binarizer.transform(pred_proba_1)

get_clf_eval(y_test, custom_predict)

rf_pred_proba_1 = rf_pred_proba[:,1].reshape(-1,1)
rf_binarizer = Binarizer(threshold = custom_threshold).fit(pred_proba_1)
rf_custom_predict = rf_binarizer.transform(rf_pred_proba_1)

get_clf_eval(y_test, rf_custom_predict)
```

    오차 행렬
    [[104  13]
     [ 20  42]]
     정확도 : 0.8156424581005587 정밀도 : 0.7636363636363637 재현율 : 0.6774193548387096
    오차 행렬
    [[103  14]
     [ 14  48]]
     정확도 : 0.8435754189944135 정밀도 : 0.7741935483870968 재현율 : 0.7741935483870968



```python
from sklearn.preprocessing import Binarizer

custom_threshold = 0.2

pred_proba_1 = pred_proba[:,1].reshape(-1,1)
binarizer = Binarizer(threshold = custom_threshold).fit(pred_proba_1)
custom_predict = binarizer.transform(pred_proba_1)

get_clf_eval(y_test, custom_predict)

rf_pred_proba_1 = rf_pred_proba[:,1].reshape(-1,1)
rf_binarizer = Binarizer(threshold = custom_threshold).fit(pred_proba_1)
rf_custom_predict = rf_binarizer.transform(rf_pred_proba_1)

get_clf_eval(y_test, rf_custom_predict)
```

    오차 행렬
    [[78 39]
     [10 52]]
     정확도 : 0.7262569832402235 정밀도 : 0.5714285714285714 재현율 : 0.8387096774193549
    오차 행렬
    [[78 39]
     [ 7 55]]
     정확도 : 0.7430167597765364 정밀도 : 0.5851063829787234 재현율 : 0.8870967741935484


### recall_curve


```python
from sklearn.metrics import precision_recall_curve

pred_proba_class1 = lr_clf.predict_proba(X_test)[:,1]

precisions, recalls, thresholds = precision_recall_curve(y_test , pred_proba_class1)
print('반환된 분류 결정 임계값 배열의 shape :', thresholds.shape)
print('반환된 precisions 배열의 shape :', precisions.shape)
print('반환된 recalls 배열의 shape :', recalls.shape)

print('thresholds 5 sample : ', thresholds[:5])
print('precisions 5 sample : ', precisions[:5])
print('recalls 5 sample : ', recalls[:5])

thr_index = np.arange(0, thresholds.shape[0],15)
print('샘플 추출을 위한 임계값 배열의 index 10개 :',thr_index)
print('샘플용 10개의 임계값 : ', np.round(thresholds[thr_index],2))

print('샘플 임계값별 정밀도', np.round(precisions[thr_index],3))
print('샘플 임계값별 재현율', np.round(recalls[thr_index],3))

```

    반환된 분류 결정 임계값 배열의 shape : (166,)
    반환된 precisions 배열의 shape : (167,)
    반환된 recalls 배열의 shape : (167,)
    thresholds 5 sample :  [0.03167367 0.03495072 0.04388743 0.04565402 0.05212856]
    precisions 5 sample :  [0.34831461 0.34463277 0.34659091 0.34857143 0.35057471]
    recalls 5 sample :  [1.         0.98387097 0.98387097 0.98387097 0.98387097]
    샘플 추출을 위한 임계값 배열의 index 10개 : [  0  15  30  45  60  75  90 105 120 135 150 165]
    샘플용 10개의 임계값 :  [0.03 0.09 0.14 0.15 0.17 0.2  0.25 0.4  0.65 0.76 0.89 0.97]
    샘플 임계값별 정밀도 [0.348 0.374 0.405 0.442 0.505 0.564 0.641 0.73  0.792 0.935 1.    1.   ]
    샘플 임계값별 재현율 [1.    0.984 0.968 0.919 0.903 0.855 0.806 0.742 0.613 0.468 0.258 0.016]



```python
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
%matplotlib inline

def precision_recall_curve_plot(y_test , pred_proba_c1):
    precisions, recalls, thresholds = precision_recall_curve(y_test , pred_proba_c1)

    plt.figure(figsize = (8,6))
    threshold_bounduary = thresholds.shape[0]
    plt.plot(thresholds, precisions[0:threshold_bounduary],linestyle= '--', label= 'precision')
    plt.plot(thresholds, recalls[0:threshold_bounduary],label = 'reacall')

    start, end = plt.xlim()
    plt.xticks(np.round(np.arange(start,end,0.1),2))
    plt.xlabel('Threshold value');plt.ylabel('Precision and Recall Value')
    plt.legend();plt.grid()
    plt.show()

precision_recall_curve_plot(y_test,lr_clf.predict_proba(X_test)[:,1])
precision_recall_curve_plot(y_test,rf_clf.predict_proba(X_test)[:,1])
```


![png](\github_img\titanic\output_12_0.png)
    




![png](\github_img\titanic\output_12_1.png)
    


### roc_curve , roc_auc_score

#### sklearn.metrics.roc_curve
* sklearn.metrics.roc_curve(y_true, y_score, *, pos_label=None, sample_weight=None, drop_intermediate=True)

#### sklearn.metrics.roc_auc_score
* sklearn.metrics.roc_auc_score(y_true, y_score, *, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)


```python
from sklearn.metrics import roc_curve, roc_auc_score

def roc_curve_plot(y_test, pred_proba_c1,rf_pred_proba_c1):
    #임곗값에 따른 FPR, TPR값을 반환 받음
    fprs, tprs, thresholds = roc_curve(y_test, pred_proba_c1)
    rf_fprs, rf_tprs, rf_thresholds = roc_curve(y_test, rf_pred_proba_c1)
    #ROC Curve를 plot 곡선으로 그림
    plt.plot(fprs, tprs, label='ROC')
    plt.plot(rf_fprs, rf_tprs,'b', label='rf_ROC')
    #가운데 대각선 직선을 그림
    plt.plot([0,1], [0,1], 'k--', label='Random')

    #FPR X축의 Scale을 0.1 단위로 변경,  X,Y 축명 설정 등
    start, end = plt.xlim()
    plt.xticks(np.round(np.arange(start,end,0.1),2))
    plt.xlim(0,1); plt.ylim(0,1)
    plt.xlabel('FPR(1-Sensitivity)'); plt.ylabel('TPR(Recall)')
    plt.legend()
    plt.show()
roc_curve_plot(y_test, lr_clf.predict_proba(X_test)[:, 1],rf_clf.predict_proba(X_test)[:, 1])
```


![png](\github_img\titanic\output_16_0.png)
    

